### Energy based model

Energy based probabilistic models define a probability distribution through an energy function:

$$
p(\vec x) = \frac{e^{-E(\vec x)}}{Z}
$$

where \\(Z\\) is the normalization factor, which is also called the partition function by analogy with 
physical systems:

$$
Z = \sum_{\vec x} e^{-E(\vec x)}
$$

The formulae looks pretty much like the one of softmax.

An energy based model can be learnt by performing sgd on the empirical negative log-likelihood of the 
training data. As for the logistic regression we will first define the log-likelihood and then the loss 
function as being the negative log-likelihood:

$$
L(\theta, D) = \frac{1}{N} \sum_{x^{(i)} \in D} \log p(x^{(i)})
$$

$$
l(\theta, D) = -L(\theta, D)
$$

### EBM with hidden units

In some situation, we may not observe \\(\vec x\\) fully, or we  want to introduce some unobserved 
variables to incrrease thee expressive power of the model. By adding the hidden variables \\(\vec h\\), 
we have:

$$
P(\vec x) = \sum_{\vec h} P(\vec x, \vec h) = \sum_{\vec h} \frac{e^{-E(\vec x, \vec h)}}{Z}
$$

Now let's introduce the notation of _free energy_, term from physics, defined as

$$
F(\vec x) = -\log \sum_{\vec h} e^{-E(\vec x, \vec h)}
$$

Then we have:

$$
P(\vec x) = \frac{e^{-F(\vec x)}}{Z}
$$

where \\(Z = \sum_{\vec x} e^{-E(\vec x, \vec h)}\\) is again the partition function.

The data negative log-likelihood gradient, the gradient of the loss, has the form:

$$
-\frac{\partial \log p(\vec x)}{\partial \theta} = \frac{\partial F(\vec x)}{\partial \theta} - \sum_{\vec x} p(\tilde x)\frac{\partial F(\tilde x)}{\parital \theta}
$$

The above gradient contains two parts, which are referred to as the positive phase and the negative 
phase. The positive phase increases the probability of training data (by reducing the corresponding 
free energy), while the negative phase decreases the probability of samples generated by the model.

It's difficult to determine the gradient analytically, as it involves the computation of 
\\(E_P [\frac{\partial F(\vec x)}{\partial \theta}]\\). 

The first stemp in making this computation tractable is to estimate the expectation using a fixed 
number of model samples. Samples used to estimate the negative phase gradient are referred to as 
negative particles, which are denoted as \\(N\\). The gradient becomes:

$$
-\frac{\partial p(\vec x)}{\partial \theta} \approx \frac{\partial F(\vec x)}{\partial \theta} - \frac{1}{||N||} \sum_{\tilde x \in N} \frac{\partial F(\tilde x)}{\partial \theta}
$$

The elements \\(\tilde x\\) of \\(N\\) are sampled according to \\(P\\) (Monte-Carlo).

### Restricted Boltzmann Machines

Boltzmann machines are a particular form of log-linear Markov Random Field, for which the energy 
function is linear in its free parameters. To make them powerful enough to represent complicated 
distributions (go from the limited parametric setting to a non-parameteric one), let's consider 
that some of the variables are never observed. Restricted Boltzmann machines restrict BMs to those 
without visible-visible and hidden-hidden connections.

The energy funciton \\(E(\vec v, \vec h)\\) of an RBM is defined as:

$$
E(\vec v, \vec h) = -\vec b ^T \vec v - \vec c^T \vec h - \vec h ^T \Omega \vec v
$$

where \\(\Omega\\) represents the weights connecting hidden and visible units and \\(\vec b\\) and 
\\(\vec c\\) are the offsets of the visible and hidden variables respectively.

Thus the energy function:

$$
F(\vec v) = -\vec b^T \vec v - \sum_i \log \sum_{\vec h_i} e ^ {h_i (c_i + \Omega_i \vec v)}
$$

The visible and hidden units are conditionally independent given one-another. So we have:

$$
p(\vec h | \vec v) = \prod_i p(h_i | \vec v)
$$

$$
p(\vec v | \vec h) = \prod_i p(v_i | \vec h)
$$

#### RBM with binary units

Suppose that \\(\vec v\\) and \\(\vec h\\) are binary vectors, a probabilistic version of the 
usual neuron activation function turns out to be:

$$
P(h_i = 1 | \vec v) = \sigm (c_i + \Omega_i \vec v)
$$

$$
P(v_i = 1 | \vec h) = \sigm (b_i + \Omega_i \vec h)
$$

The free energy of an RBM with binary units further simplifies to:

$$
F(\vec v) = -\vec b^T \vec v - \sum_i \log (1 + e^(c_i + \Omega_i\vec v))
$$

And the gradients for an RBM with binary units:

$$
-\frac{\partial \log p(\vec v)}{\partial \Omega_{ij}} = E_{\vec v} [p(h_i | \vec v) v_j] - v_j^{(i)} \sigm(\Omega_i \vec v^{(i)} + c_i)
$$
$$
-\frac{\partial \log p(\vec v)}{\partial c_i} = E_{\vec v} [p(h_i | \vec v)] - \sigm(\Omega_i \vec^{(i)})
$$
$$
-\frac{\partial \log p(\vec v)}{\partial b_j} = E_{\vec v} [p(v_j | \vec h)] - v_j^{(i)}
$$
